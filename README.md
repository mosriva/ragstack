# ğŸ¢ RAGStack 
*Reference implementation of **RAGStack: A Privacy-First GenAI Retrieval-Augmented Generation Architecture for Secure Enterprise Document Intelligence***  

<p align="center">
  <a href="https://github.com/mosriva/ragstack/stargazers"><img src="https://img.shields.io/github/stars/mosriva/ragstack?style=flat-square&color=yellow" alt="GitHub Stars"></a>
  <a href="https://github.com/mosriva/ragstack/releases"><img src="https://img.shields.io/github/v/release/mosriva/ragstack?style=flat-square&color=blue" alt="Latest Release"></a>
  <a href="https://zenodo.org/badge/latestdoi/zenodo.XXXXXXX"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.XXXXXXX.svg" alt="DOI"></a>
  <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-green.svg?style=flat-square" alt="License: MIT"></a>
  <a href="https://techrxiv.org"><img src="https://img.shields.io/badge/Preprint-TechRxiv-orange?style=flat-square" alt="TechRxiv"></a>
</p>

---

## ğŸ“˜ Overview
The **RAGStack (v1.0.0)** is the reference implementation of the **RAGStack** architecture described in the paper  
â€œ*RAGStack: A Privacy-First GenAI Retrieval-Augmented Generation Architecture for Secure Enterprise Document Intelligence*â€.
It combines **Ollama**, **FAISS**, and **Streamlit** to deliver an air-gapped GenAI workflow â€” ensuring **data privacy**, **cloud-free execution**, and **reproducibility**.
This project demonstrates how modular, open-source GenAI architectures can power internal enterprise search, compliance review, and knowledge-retrieval systems.

---

## ğŸš€ Key Features
- ğŸ“ Automatic PDF ingestion and text chunking  
- ğŸ” Contextual retrieval using FAISS vector search  
- ğŸ§  Local LLMs via **Ollama** (Mistral, LLaMA2, Phi)  
- ğŸ’¬ Streamlit-based conversational interface  
- ğŸ§¾ Persistent chat history + CSV export  
- ğŸ—‚ï¸ Self-healing FAISS index (auto-rebuilt if missing)  
- ğŸ”„ SHA-1 deduplication and manifest tracking  
- ğŸ“ˆ Normalized **similarity scoring** (0 â€“ 1) for intuitive relevance display  

---

## ğŸ§± System Architecture

The high-level architecture of RAGStack is shown below, illustrating ingestion, vectorization, retrieval, and local reasoning flow.


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RAGStack: Privacy-First Enterprise GenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                  (Air-Gapped / On-Prem / No External APIs / Full Data Sovereignty)
         
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

<pre>

DOCUMENT INGESTION & VECTORIZATION                   QUERY, RETRIEVAL & LOCAL REASONING

</pre>


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      PDF Documents       â”‚                       â”‚        User Query        â”‚
    â”‚  Uploaded via Streamlit  â”‚                       â”‚   Streamlit Q&A Form     â”‚
    â”‚  Rebuild Index if Missingâ”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
                  â”‚                                                  â–¼
                  â–¼                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚     Query Embeddings     â”‚
    â”‚    PyMuPDF Parsing       â”‚                     â”‚   SentenceTransformers   â”‚
    â”‚    300-word Chunking     â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
                  â”‚                                                â–¼
                  â–¼                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚   FAISS Top-K Retrieval  â”‚
    â”‚  SentenceTransformers    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Normalized Similarity   â”‚
    â”‚  all-MiniLM-L6-v2        â”‚   Nearest Neighbors â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
                  â”‚                                                â–¼
                  â–¼                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚    Prompt Construction   â”‚
    â”‚     FAISS Vector Index   â”‚                     â”‚ Context + Filename + Pg  â”‚
    â”‚     FlatL2 (Persistent)  â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
                  â”‚                                                â–¼
                  â–¼                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  Answer + Attribution    â”‚
    â”‚      Persistent Storage & Audit Logs     â”‚â—„â”€â”€â”€â”€â”¤ Streamlit UI + Export    â”‚
    â”‚ uploaded_pdfs / index / logs directories â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚        Local LLM Runtime         â”‚
                                   â”‚  Ollama (Mistral / LLaMA2 / Phi) â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   Health & Governance Panel      â”‚
                                   â”‚ â€¢ Ollama API & Model Status      â”‚
                                   â”‚ â€¢ Index & Document Counters      â”‚
                                   â”‚ â€¢ De-duplication Manifest        â”‚
                                   â”‚ â€¢ Exportable Audit Logs (CSV)    â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---
## âš™ï¸ Components and Workflow

| Component | Description | Library / Tool |
|------------|--------------|----------------|
| **PDF Parser** | Extracts text and metadata | PyMuPDF |
| **Chunking Module** | Splits text into coherent blocks | Native loop (optional LangChain splitter) |
| **Embedding Generator** | Converts text to vector representation | `sentence-transformers` |
| **Vector Store** | Stores / retrieves embeddings | FAISS |
| **Retriever** | Retrieves top-k chunks using **normalized similarity (1 / (1 + distance))** | FAISS |
| **Local LLM** | Generates grounded answers | Ollama (Mistral / LLaMA2 / Phi) |
| **UI Layer** | Uploads, chat, export | Streamlit |

**Workflow Summary**
1. Upload one or more PDFs via the Streamlit UI.  
2. Text is parsed, chunked, and embedded locally.  
3. A query retrieves the top-k most similar chunks.  
4. Context is appended to the LLM prompt for grounding.  
5. Answers + sources are displayed and logged.

---

## ğŸ§© Technical Stack
- **Language:** Python 3.10 +  
- **Frameworks:** Streamlit, FAISS, SentenceTransformers  
- **LLM Runtime:** Ollama (Mistral, LLaMA2, Phi)  
- **Persistence:** Local files for PDFs, FAISS index, and chat logs  
- **Platform:** Offline / air-gapped enterprise environments  

---

## ğŸ§° Installation & Setup
1. **Clone the Repository**
```
   git clone https://github.com/mosriva/ragstack.git
   cd ragstack
```
2. Create & Activate Virtual Environment (Recommended)
```
python3 -m venv rag_venv
source rag_venv/bin/activate    # macOS / Linux
# .\rag_venv\Scripts\activate # Windows PowerShell
```
3. Install Dependencies
```
pip install --upgrade pip
pip install -r requirements.txt
```
4. Start Ollama and Load Model
```
ollama pull mistral
ollama run mistral
```
5. Launch Streamlit App
```
streamlit run streamlit_ui.py --server.port 8501
```
ğŸ“Œ Optional (GPU Acceleration & Alternative Models)
```
ollama pull llama2   # or phi, mistral:latest, etc. To switch models, use the dropdown in the Streamlit UI.
#âš  If switching models (e.g., phi, llama2), ensure that model name matches exactly in Streamlit dropdown and Ollama supports the model locally.
```
ğŸ”„ Uninstall / Environment Reset
```
pip uninstall -y torch sentence-transformers streamlit faiss-cpu pymupdf pandas
rm -rf rag_venv index uploaded_pdfs logs

# Optional: Remove cached Ollama models if needed
# ollama rm mistral; ollama rm llama2; ollama rm phi
```
After uninstall/reset, recreate the environment starting from Step 2.


ğŸ–¥ï¸ macOS Users â€” Important Runtime Fix (Avoid Segmentation Fault)

Some macOS systems (especially M1/M2/M3 chips) may experience segmentation faults when running Streamlit with PyTorch, FAISS, and HuggingFace tokenizers together.
This is a known issue related to macOS fork safety + parallel tokenizers.

To ensure stable execution, set the following environment variables before launching the app:
```
export TOKENIZERS_PARALLELISM=false
export OMP_NUM_THREADS=1
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
```

Then start the application:
```
streamlit run streamlit_ui.py --server.port 8501
```
Why this is needed?

TOKENIZERS_PARALLELISM=false
Prevents HuggingFace tokenizers from spawning threads too early (avoids deadlocks/segfaults).

OMP_NUM_THREADS=1
Reduces thread contention inside PyTorch and FAISS on macOS.

OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
Allows PyTorch + tokenizers to run safely after macOS performs a fork (Streamlit internally forks processes).

## ğŸ§ª Example Interaction

**ğŸ“ Query:**  
> What are the key risks mentioned in the data governance policy?

**ğŸ’¡ Response (sample):**  
The policy highlights risks related to unauthorized access, data-retention violations, and compliance failures.

**ğŸ“ Sources:**  
- *data-governance.pdf*, Page 4  
- *data-policy.pdf*, Page 1  

## ğŸ“Š Evaluation Summary
| Metric                 | Description                                 | Result (Example) |
| ---------------------- | ------------------------------------------- | ---------------- |
| Response Latency       | Avg. query time (Mistral 7B local)          | ~5-15 s           |
| Retrieval Precision    | Relevance of retrieved chunks (manual eval) | 0.89             |
| Context Token Coverage | % of retrieved text used in final prompt    | ~78 %            |
| Memory Usage           | Peak FAISS index memory (100 PDFs)          | ~180 MB          |


## ğŸ“‚ Repository Structure
```
ragstack/
â”œâ”€â”€ README.md                   # Project overview
â”œâ”€â”€ streamlit_ui.py            # Main application
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ CHANGELOG.md               # Version history
â”œâ”€â”€ CONTRIBUTING.md            # Contribution guidelines
â”œâ”€â”€ release_notes.md           # Release-specific highlights
â”œâ”€â”€ citation.cff               # Citation metadata
â”œâ”€â”€ LICENSE                    # MIT open-source license
â”œâ”€â”€ .gitignore
â”œâ”€â”€ uploaded_pdfs/
â”‚   â””â”€â”€ .gitkeep               # (ignored in git)
â”œâ”€â”€ index/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ .gitkeep
â””â”€â”€ docs/
    â””â”€â”€Architecture/  
        â””â”€â”€ ragstack_architecture_figure1.png

```
## âš–ï¸ License

This project is released under the MIT License.
You are free to use, modify, and distribute it with attribution.

## ğŸ§  Citation

If you use this work in your research, please cite:

Srivastava, M. (2025). RAGStack: A Privacy-First GenAI Retrieval-Augmented Generation Architecture for Secure Enterprise Document Intelligence. Zenodo. https://doi.org/10.5281/zenodo.XXXXXXX

## ğŸ™Œ Acknowledgments & Inspiration

Ollama â€“ Local LLM runtime for private inference

FAISS â€“ Efficient vector similarity search

LangChain / LlamaIndex â€“ RAG design patterns

Streamlit â€“ Rapid UI prototyping for GenAI

## ğŸ”— Related Links

TechRxiv Preprint: [Link to be added once published]

Zenodo Implementation Archive: [Link to be added DOI once issued]

GitHub Repository: https://github.com/mosriva/ragstack
